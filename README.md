# unruly-nightmares
Using NLP techniques and LLMs to mimic style and conversation
## Synopsis
This project is an exploration of Natural Language Processing techniques, pretrained large language models, and how they can be used to mimic writing style and interact with a literary work.
Here‚Äôs what the code does:
* Have a ‚Äúconversation‚Äù with the poet Rub√©n Dar√≠o through his book Autobiograf√≠a ([Retrieval-Augmented Generation](https://github.com/cepedayan/unruly-nightmares/blob/main/retrieval_augmented_generation/Retrieval_Augmented_Generation.ipynb)).
* Find the most similar sentence in any given book and use it to mimic conversation ([Sentence Similarity](https://github.com/cepedayan/unruly-nightmares/blob/main/sentence_similarity/Sentence_Similarity.ipynb)).
* Replicate some of Gustavo Adolfo Becquer‚Äôs most prominent stylistic attributes ([Finetuning & encoding methods for text generation](https://github.com/cepedayan/unruly-nightmares/tree/main/finetuning)).

This project was developed for the KaggleX program. Special thanks to my mentor, Jonathan Schlosser, who not only guided me with their expertise in NLP, but also changed my perspective regarding my career in Data Science and had a tremendous impact on my own personal outlook and capabilities.

Although a lot more can be done ‚Äîmeasure quality and compare model performance with linguistic KPIs, prompt engineering, experiment with number of epochs and track training loss‚Äî, this is the start of my journey with NLP.

### Example outputs
If you prefer to skip directly to the results, see here #pending.

## My experience
My starting point was a basic knowledge of Python and NLP concepts, and a very keen interest in understanding this marvelous technology. I wanted to learn how LLMs understand natural language, how they process it, and discover practical applications in creative spaces.

As a Localization Editor for Spanish in the Games industry, I seek to understand how LLMs can aid creative roles such as writers, translators, and editors in their daily tasks, in particular, those that relate to maintaining the voice and style of the brand and ensuring the continuity of characters and lore as valuable intellectual property of a company.
The questions that prompted this project were:
* How can LLMs learn stylistic attributes present in reference documents and replicate them in newly generated text?
* Can style be measured quantitatively with the help of an LLM, removing the editor‚Äôs subjective evaluation from the equation?
* What are the techniques used to make LLMs learn style vs learn facts present in any given document?

## The datasets
The following books were downloaded, cleaned, and preprocessed for using in various NLP techniques. (Public domain for their use in the US, gutemberg.org) 
* _Leyendas, cuentos y poemas_, by Gustavo Adolfo B√©cquer
* _Obras escogidas_, by Gustavo Adolfo B√©cquer
* _Autobiograf√≠a_, by Rub√©n Dar√≠o
  
## What I learned:
* About style ([See code here](https://github.com/cepedayan/unruly-nightmares/tree/main/finetuning)):
  * Stylistic attributes can be replicated by finetuning with a dataset in the target writing style. Attributes such as replication of imagery, specific typography not common in modern languages, average sentence length, register and tone, can all be learned by an open-source model with less than 1M characters.
  * The output can be greatly improved using various decoding methods‚Äîthe best performance in this case was obtained with Beam search and Sampling.
    * I used DeepESP/gpt2-spanish, available in Hugging Face, and a dataset of ~750K characters.
* About having a ‚Äúconversation‚Äù with a poet through their work:
  * Models that are pretrained for the Sentence Similarity task can be used to retrieve the source text that is most similar to the sentence fed in the prompt. This by no means resemble a conversation, but it‚Äôs a starting point. ([See code here](https://github.com/cepedayan/unruly-nightmares/tree/main/sentence_similarity))
  * Retrieval-Augmented Generation is a really powerful way to answer questions based on documents that are specific to any given task or industry. In combination with a powerful LLM, such as gpt-3.5-turbo, we get a really cool text generator, conversation partner, assistant‚Äîwhatever we need, just give the instruction in the prompt!‚Äî with access to our knowledge repository. ([See code here](https://github.com/cepedayan/unruly-nightmares/tree/main/retrieval_augmented_generation))
    * Although we should beware of hallucinations‚Äîthis can be somewhat offset by changing the temperature‚Äî, the advantage of RAG is that it allows us to track answers back to their original source (if present in the source documents). This is an advantage vs using the typical LLM chat platforms. The cost of embedding source documents and queries is an important consideration that should be weighed against the increase in productivity and resources required.
* About myself:
  * I possess the ability to ride steep learning curves‚Äîit‚Äôs quite amazing to think that the knowledge I started with was a vague idea of what a token was and that ChatGPT was a LLM.
  * Resilience is the biggest asset in the face of complicated problems‚Äîmy ability to find answers and alternative solutions was put to the test after so many errors I had to solve. (See the error log here for your amusement.) #pending
  * NLP requires a combination of creative and logical approach to problems‚Äîmy preferred type of challenge.
  * I learned to value my skills over the knowledge I have at any given time‚Äîproblem-solving and communication is crucial.

## Libraries and techniques
Python libraries:
* numpy
* operating system (os)
* regex (re)
* scikit-learn

NLP techniques:
* Tokenization
* Finetuning
* Embeddings
* Decoding methods for text generation
* Prompt engineering
* Retrieval-augmented generation

NLP libraries and tools:
* Transformers (Hugging Face)
* chromadb

Deep learning and LLM frameworks:
* Torch
* TensorFlow
* LangChain
* OpenAI API

Others:
* Character encoding standards

## Output examples

These are some of the resulting "conversations" with each of the models and NLP techniques used.

### Finetuning (gpt2-spanish finetuned)
Prompt:
`"Ser√° verdad que..."`

Output:
`Ser√° verdad que no se ha de
hablar de la verdad, y que no se ha de
haber de la verdad, y que no se ha de
haber de la verdad, sino de la verdad, y que
se ha de creer que no se ha de creer en nada, y que
se ha de creer que no se ha de creer en nada, y que
se ha de creer en algo, y que no se ha de creer en nada, y que
se ha de creer en algo, y que no se ha de creer en nada, y que
se ha de creer en algo, y que no se ha de creer en nada, y que
se ha de creer en algo, y que se`

### Decoding methods (Sampling, gpt2-spanish finetuned)
Model output:

`Mientras flotaban en el aire la luz de la luna. La brisa comenzaba √° refrescar el silencio; los √°rboles se pon√≠an en movimiento. Los p√°jaros cantaban, √° la luz de las l√°mparas, el viento de un
recuerdo sobre las hojas que se mov√≠an bajo sus pies. Entre tanto, el viento se hab√≠a hecho m√°s
poco en el`

Fragment from original book for comparison purposes:

`De todos modos, all√° van estas cuartillas, valgan por lo
que valieren: que si alguien de m√°s conocimientos √© importancia,
una vez apuntada la idea, la desarrolla y prepara la opini√≥n para
que fructifique, no ser√°n perdidas del todo. Yo, entre tanto, voy √°
trazar un tipo bastante original y que desconf√≠o de poder reproducir.
Ya que no de otro modo, y aunque poco valga, contribuir√© al √©xito de
la predicaci√≥n con el ejemplo.`

### Sentence Similarity (google/flan-t5/xxl)
Human.- `¬øC√≥mo se llama la persona m√°s agradable que conoce?`
Autobiograf√≠a.txt.- `Era un amable y jovial fil√≥sofo.`

Human.- `¬øC√≥mo se llamaba?`
Autobiograf√≠a.txt.- `Se llamaba con un nombre balzaciano, Sebasti√°n Menmolth.`

Human.- `¬øC√≥mo describir√≠a su personalidad?`
Autobiograf√≠a.txt.- `Escrib√≠a una prosa profusa, llena de vitalidad y de color, de plasticidad y de m√∫sica.`

Human.- `¬øQu√© le gusta hacer a usted, adem√°s de la escritura?`
Autobiograf√≠a.txt.- `Extra√±a y ardua mezcla de cosas para la cabeza de un ni√±o.`

Human.- `¬øMe puede dar un ejemplo?`
Autobiograf√≠a.txt.- `¬øPara d√≥nde? Para cualquier parte.`

### Retrieval-Augmented Generation (gpt-3.5-turbo)

Emoji coding:
  ‚õÑ `temperature = 0.7`
  
  ü•µ `temperature = 1`
  
  ‚úÖ Correct answer, cross-checked with source document
  
  ‚ùå Incorrect answer, cross-checked with source document

Human.-	`Buen d√≠a, ¬øme permite un minuto para hacerle unas preguntas?`
‚õÑ gpt-3.5-turbo.-	`Buen d√≠a, estimado interlocutor. Por supuesto, estoy dispuesto a concederle un minuto de mi valioso tiempo para escuchar sus preguntas y brindarle las respuestas que est√© en mi capacidad proporcionar. Adelante, estoy todo o√≠dos.`

Human.-	`¬øQu√© reuniones literarias era las que m√°s disfrutaba?`
‚õÑ gpt-3.5-turbo.-	`Las reuniones literarias que m√°s disfrutaba eran las que se llevaban a cabo en la casa de don Rafael Obligado.` ‚úÖ

Human.-	`¬øCu√°l era su relaci√≥n con don Rafael y c√≥mo describir√≠a su personalidad?`
‚õÑ gpt-3.5-turbo.-	`Mi relaci√≥n con don Rafael era de amistad y admiraci√≥n mutua. Don Rafael era un hombre de gran car√°cter y determinaci√≥n. Era un l√≠der pol√≠tico apasionado y comprometido con sus ideales. Su personalidad era en√©rgica y carism√°tica, capaz de cautivar a las masas con su elocuencia y convicci√≥n. Era un hombre de principios firmes y no tem√≠a enfrentarse a los desaf√≠os que se le presentaban. Su valent√≠a y determinaci√≥n lo llevaron a sellar su historia con su tr√°gico final.` ‚ùå

Human.-	`¬øQu√© representa la poes√≠a para usted?`
ü•µ gpt-3.5-turbo.-	`La poes√≠a, para m√≠, es como el susurro del viento entre las hojas de los √°rboles, es la melod√≠a de los p√°jaros al amanecer y el aroma embriagador de las rosas en plena floraci√≥n. Es el lenguaje de los sentimientos profundos y de las emociones indescriptibles. Representa una ventana hacia lo inefable, una forma de expresar lo inexpresable. La poes√≠a es un b√°lsamo para el alma, un refugio en medio del caos, un remanso de paz en el agitado mar de la vida. ¬øY para usted, qu√© representa la poes√≠a?` ‚úÖ (At least, the answer resembles the imagery present in the source document: `Po√©tica, s√≠, para quien sepa comprender el vaho de arte que flota sobre ese inconmesurable oc√©ano de tierra, sobre todo en los crep√∫sculos vespertinos y en los amaneceres.`)
‚õÑ gpt-3.5-turbo.-	`La poes√≠a para m√≠ representa una ventana hacia la belleza y la profundidad de las emociones humanas. Es un lenguaje que va m√°s all√° de las palabras cotidianas y nos permite explorar los misterios del alma. A trav√©s de la poes√≠a, podemos capturar momentos ef√≠meros y transmitirlos de manera eterna. Es un medio para expresar nuestras experiencias m√°s √≠ntimas y conectarnos con el mundo que nos rodea de una manera √∫nica y trascendental. ¬øCu√°l es tu poema favorito y por qu√©?` ‚ùå

Human.-	`Adoro 'Los motivos del lobo', lo aprend√≠ para recitarlo en la escuela.`
‚õÑ gpt-3.5-turbo.-	`Me alegra saber que tambi√©n te gusta "Los motivos del lobo". Es una obra maravillosa de Jos√© Emilio Pacheco. Su poes√≠a siempre ha tenido un impacto profundo en los lectores y es muy popular en las escuelas. ¬øTe gustar√≠a compartir alg√∫n otro poema que hayas aprendido para recitar en la escuela?` ‚ùå üòë
ü•µ gpt-3.5-turbo.-	`Qu√© maravilloso que hayas aprendido "Los motivos del lobo" para recitarlo en la escuela. Es uno de los poemas m√°s ic√≥nicos de mi obra y me alegra saber que lo disfrutas. ¬øHay alg√∫n otro poema que te gustar√≠a aprender o recitar?` ‚úÖ üòµ

## Error log
#pending

